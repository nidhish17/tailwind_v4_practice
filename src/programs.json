{
    "program1": "data = fetch_california_housing(as_frame=True)\nhousing_data = data.frame\n\nnumerical_features = housing_data.select_dtypes(include=[np.number]).columns\n\n# Histogram plot for all the numerical features\nplt.figure(figsize=(15,10))\ncounter = 0\nfor i, feature in enumerate(numerical_features):\n    plt.subplot(4, 4, i+1)\n    sns.histplot(housing_data[feature], kde=True, bins=30)\n    plt.title(feature)\nplt.show()\n\n# box plots\nplt.figure(figsize=(15,10))\nfor idx, feature in enumerate(numerical_features):\n    plt.subplot(3, 3, idx+1)\n    sns.boxplot(housing_data[feature], color=\"crimson\")\n    plt.title(feature)\nplt.show()\n#%%\n# identifying outliers\noutliers_summary = {}\n\nfor feature in numerical_features:\n    q1 = housing_data[feature].quantile(.25)\n    q3 = housing_data[feature].quantile(.75)\n    iqr = q3 - q1\n\n    lower_bound = (q1 - 1.5) * iqr\n    upper_bound = (q3 + 1.5) * iqr\n\n    outliers = housing_data[(housing_data[feature] < lower_bound) | (housing_data[feature] > upper_bound)]\n    outliers_summary[feature] = len(outliers)\n\nprint(outliers_summary)\n#%%\nhousing_data.describe()",
    "program2": "california_data = fetch_california_housing(as_frame=True)\ndata = california_data.frame\n\ncor_mat = data.corr()\n\nplt.figure(figsize=(15, 10))\n\nsns.heatmap(cor_mat, annot=True, cmap=\"coolwarm\")\nplt.show()\n\n# pair plot\nsns.pairplot(data, diag_kind=\"kde\")\nplt.show()",
    "program3": "iris = load_iris()\ndata = iris.data\nlabels = iris.target\nlabel_names = iris.target_names\niris_df = pd.DataFrame(data, columns=iris.feature_names)\n#%%\npca = PCA(n_components=2)\ndata_reduced = pca.fit_transform(data)\nreduced_df = pd.DataFrame(data_reduced, columns=[\"principal_component_1\", \"principal_component_2\"])\nreduced_df[\"label\"] = labels\n\n#%%\nfrom bokeh.colors.named import colors\n\n# plot the reduced data\nplt.figure(figsize=(15, 10))\nfor idx, label in enumerate(np.unique(labels)):\n    plt.scatter(\n        reduced_df[reduced_df['label'] == label]['principal_component_1'],\n        reduced_df[reduced_df[\"label\"] == label][\"principal_component_2\"],\n        label=label_names[label],\n    )\n\nplt.grid()\nplt.legend()\nplt.title(\"iris Dataset\")\nplt.show()",
    "program4": "data = read_csv(\"https://tailwindv4.netlify.app/data.csv\")\ndata\n\n\n#%%\ndef find_s_algo():\n    features = data.columns[:-1]\n    print(features)\n    y_label = data.columns[-1]\n    print(y_label)\n    hypothesis = [\"?\" for _ in features]\n    print(hypothesis)\n\n    for idx, row in data.iterrows():\n        if row[y_label] == \"yes\":\n            for i, value in enumerate(row[features]):\n                if hypothesis[i] == \"?\" or hypothesis[i] == value:\n                    hypothesis[i] = value\n                else:\n                    hypothesis[i] = \"?\"\n\n    return hypothesis\n\nfind_s_algo()",
    "program7": {
        "imports": "from sklearn.datasets import fetch_california_housing\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.pipeline import make_pipeline",
        "linear_regression": "def linear_regression_california():\n    housing = fetch_california_housing(as_frame=True)\n    X = housing.data[[\"AveRooms\"]]\n    y = housing.target\n\n    x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=69)\n\n    model = LinearRegression()\n    model.fit(x_train, y_train)\n\n    y_pred = model.predict(x_test)\n\n    plt.scatter(x_test, y_test, color=\"blue\", label=\"Actual\")\n    plt.plot(x_test, y_pred, color=\"red\", label=\"predicted\")\n    plt.legend()\n    plt.show()",
        "polynomial_regression": "def polynomial_regression():\n    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n    column_names = [\"mpg\", \"cylinders\", \"displacement\", \"sdf\", \"sdfsdf\", \"dfg\", \"ghf\", \"ddfg\"]\n\n    data = pd.read_csv(url, sep=\"\\s+\", names=column_names, na_values=\"?\")\n    data.dropna(inplace=True)\n\n\n    x = data[\"displacement\"].values.reshape(-1, 1)\n    y = data[\"mpg\"].values\n\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=69)\n\n    model = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), LinearRegression())\n\n    model.fit(x_train, y_train)\n\n    y_pred = model.predict(x_test)\n\n    plt.scatter(x_test, y_test, color=\"blue\", label=\"Actual\")\n    plt.scatter(x_test, y_pred, color=\"red\", label=\"Predicted\")\n    plt.legend()\n    plt.show()"
    },
    "program8": {
        "imports": "import matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree",
        "code": "data = load_breast_cancer()\nx = data.data\ny = data.target\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=69)\n\nmodel = DecisionTreeClassifier(random_state=69)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\npred_class = \"Benign\" if model.predict([x_test[0]]) == 1 else \"Malignant\"\nprint(f\"Predicted: {pred_class}\")\n\nplt.figure(figsize=(15,10))\nplot_tree(model, filled=True, feature_names=data.feature_names.tolist(), class_names=data.target_names.tolist())\nplt.title(\"Decision tree\")\nplt.show()\n\n\n"
    },
    "program9": "data = fetch_olivetti_faces(shuffle=True, random_state=69)\nx = data.data\ny = data.target\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=69)\nmodel = GaussianNB()\n\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\n\nfig, axes = plt.subplots(3, 5, figsize=(6, 6))\nfor ax, img, lbl, pred in zip(axes.ravel(), x_test, y_test, y_pred):\n    ax.imshow(img.reshape(64, 64), cmap=\"gray\")\n    ax.axis(\"off\")\n    ax.set_title(f\"T: {lbl}, P: {pred}\")\nplt.show()\n\n",
    "program5": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom collections import Counter\ndata = np.random.rand(100)\nlabels = [\"Class1\" if x <= 0.5 else \"Class2\" for x in data[:50]]\ndef euclidean_distance(x1, x2):\n    return abs(x1 - x2)\ndef knn_classifier(train_data, train_labels, test_point, k):\n    distances = [(euclidean_distance(test_point, train_data[i]), train_labels[i]) for i in\n    range(len(train_data))]\n    distances.sort(key=lambda x: x[0])\n    k_nearest_neighbors = distances[:k]\n    k_nearest_labels = [label for _, label in k_nearest_neighbors]\n    return Counter(k_nearest_labels).most_common(1)[0][0]\ntrain_data = data[:50]\ntrain_labels = labels\ntest_data = data[50:]\nk_values = [1, 2, 3, 4, 5, 20, 30]\nprint(\"--- k-Nearest Neighbors Classification ---\")\nprint(\"Training dataset: First 50 points labeled based on the rule (x <= 0.5 -> Class1, x > 0.5 -> Class2)\")\nprint(\"Testing dataset: Remaining 50 points to be classified\n\")\nresults = {}\nfor k in k_values:\n    print(f\"Results for k = {k}:\")\n    classified_labels = [knn_classifier(train_data, train_labels, test_point, k) for test_point in\n    test_data]\n    results[k] = classified_labels\nfor i, label in enumerate(classified_labels, start=51):\n    print(f\"Point x{i} (value: {test_data[i - 51]:.4f}) is classified as {label}\")\nprint(\"\n\")\nprint(\"Classification complete.\n\")\nfor k in k_values:\n    classified_labels = results[k]\n    class1_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == \"Class1\"]\n    class2_points = [test_data[i] for i in range(len(test_data)) if classified_labels[i] == \"Class2\"]\n    plt.figure(figsize=(10, 6))\n    plt.scatter(train_data, [0] * len(train_data), c=[\"blue\" if label == \"Class1\" else \"red\" for label\n    in train_labels],\n    label=\"Training Data\", marker=\"o\")\n    plt.scatter(class1_points, [1] * len(class1_points), c=\"blue\", label=\"Class1 (Test)\",\n    marker=\"x\")\n    plt.scatter(class2_points, [1] * len(class2_points), c=\"red\", label=\"Class2 (Test)\", marker=\"x\")\n    plt.title(f\"k-NN Classification Results for k = {k}\")\n    plt.xlabel(\"Data Points\")\n    plt.ylabel(\"Classification Level\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()",
    "program6": "import matplotlib.pyplot as plt\nimport numpy as np\ndef gaussian_kernel(x, xi, tau):\n    return np.exp(-np.sum((x - xi) ** 2) / (2 * tau ** 2))\ndef locally_weighted_regression(x, X, y, tau):\n    m = X.shape[0]\n    weights = np.array([gaussian_kernel(x, X[i], tau) for i in range(m)])\n    W = np.diag(weights)\n    X_transpose_W = X.T @ W\n    theta = np.linalg.inv(X_transpose_W @ X) @ X_transpose_W @ y\n    return x @ theta\nnp.random.seed(42)\nX = np.linspace(0, 2 * np.pi, 100)\ny = np.sin(X) + 0.1 * np.random.randn(100)\nX_bias = np.c_[np.ones(X.shape), X]\nx_test = np.linspace(0, 2 * np.pi, 200)\nx_test_bias = np.c_[np.ones(x_test.shape), x_test]\ntau = 0.5\ny_pred = np.array([locally_weighted_regression(xi, X_bias, y, tau) for xi in x_test_bias])\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='red', label='Training Data', alpha=0.7)\nplt.plot(x_test, y_pred, color='blue', label=f'LWR Fit (tau={tau})', linewidth=2)\nplt.xlabel('X', fontsize=12)\nplt.ylabel('y', fontsize=12)\nplt.title('Locally Weighted Regression', fontsize=14)\nplt.legend(fontsize=10)\nplt.grid(alpha=0.3)\nplt.show()",
    "program10": "import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, classification_report\ndata = load_breast_cancer()\nX = data.data\ny = data.target\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nkmeans = KMeans(n_clusters=2, random_state=42)\ny_kmeans = kmeans.fit_predict(X_scaled)\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y, y_kmeans))\nprint(\"\nClassification Report:\")\nprint(classification_report(y, y_kmeans))\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X_scaled)\ndf = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\ndf['Cluster'] = y_kmeans\ndf['True Label'] = y\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black',\nalpha=0.7)\nplt.title('K-Means Clustering of Breast Cancer Dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title=\"Cluster\")\nplt.show()\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='PC1', y='PC2', hue='True Label', palette='coolwarm', s=100,\nedgecolor='black', alpha=0.7)\nplt.title('True Labels of Breast Cancer Dataset')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title=\"True Label\")\nplt.show()\nplt.figure(figsize=(8, 6))\nsns.scatterplot(data=df, x='PC1', y='PC2', hue='Cluster', palette='Set1', s=100, edgecolor='black',\nalpha=0.7)\ncenters = pca.transform(kmeans.cluster_centers_)\nplt.scatter(centers[:, 0], centers[:, 1], s=200, c='red', marker='X', label='Centroids')\nplt.title('K-Means Clustering with Centroids')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend(title=\"Cluster\")\nplt.show()"
}